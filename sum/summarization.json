{
  "updated_at": "2025-01-23T07:26:23+00:00",
  "folder_id": "3dd71a86-d1ac-44a8-82be-c2ae075c3745",
  "webhook": false,
  "description": "Crafting Conversations, One Node at a Time.",
  "endpoint_name": null,
  "name": "summarization",
  "id": "9960bfb7-b5ea-443b-8e4e-244837962554",
  "icon_bg_color": null,
  "data": {
    "nodes": [
      {
        "id": "ChatInput-kaAiz",
        "type": "genericNode",
        "position": {
          "x": -444.15599999999984,
          "y": 44.35200000000003
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "files": {
                "trace_as_metadata": true,
                "file_path": "",
                "fileTypes": [
                  "txt",
                  "md",
                  "mdx",
                  "csv",
                  "json",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "pdf",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx",
                  "jpg",
                  "jpeg",
                  "png",
                  "bmp",
                  "image"
                ],
                "list": true,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "files",
                "value": "",
                "display_name": "Files",
                "advanced": true,
                "dynamic": false,
                "info": "Files to be sent with the message.",
                "title_case": false,
                "type": "file",
                "_input_type": "FileInput"
              },
              "background_color": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "background_color",
                "value": "",
                "display_name": "Background Color",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The background color of the icon.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "chat_icon": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "chat_icon",
                "value": "",
                "display_name": "Icon",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The icon of the message.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_USER, MESSAGE_SENDER_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"MessagesSquare\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        _background_color = self.background_color\n        _text_color = self.text_color\n        _icon = self.chat_icon\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n            properties={\"background_color\": _background_color, \"text_color\": _text_color, \"icon\": _icon},\n        )\n        if self.session_id and isinstance(message, Message) and self.should_store_message:\n            stored_message = self.send_message(\n                message,\n            )\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "atient Document: Tuberculosis (TB) Management\nPatient Information\nName: Amina B.\nAge: 28\nGender: Female\nDate of Birth: 1997-02-15\nDate: 2025-01-22\nHealthcare Provider: Dr. Samira Nasri\n\nIntroduction\nTuberculosis (TB) is a bacterial infection primarily affecting the lungs, though it can spread to other organs. It is caused by Mycobacterium tuberculosis and is transmitted through the air when an infected person coughs, sneezes, or speaks. While TB is preventable and treatable, early detection and adherence to prescribed treatments are critical to prevent its spread and avoid complications.\n\nThis document outlines the management of tuberculosis, including diagnosis, treatment, and prescription information.\n\nSymptoms of Tuberculosis\nCommon symptoms of active TB include:\n\nPersistent cough (lasting more than three weeks)\nCoughing up blood or sputum\nNight sweats\nFever and chills\nWeight loss\nFatigue\nChest pain\nDiagnosis\nA healthcare provider will diagnose TB based on:\n\nMedical history and symptoms\nPhysical examination\nChest X-ray: showed bilateral infiltrates with cavitary lesions in the upper lobes.\nTuberculin skin test (TST): Positive with 15mm induration.\nSputum test: Positive for Mycobacterium tuberculosis, culture confirmed.\nBlood tests: Interferon-Gamma Release Assay (IGRA) positive.\nTreatment for Tuberculosis\nTreatment for tuberculosis involves a combination of antibiotics over a prolonged period. The standard treatment regimen for drug-susceptible TB is typically 6 months, but it may vary based on individual factors such as the presence of multi-drug resistance.\n\nFirst-Line Drugs for TB Treatment:\n\nIsoniazid (INH) – 10 mg/kg (max 300 mg) per day\nTaken daily for the first 2 months and can be extended for maintenance for up to 6 months.\nRifampin (RIF) – 10 mg/kg (max 600 mg) per day\nOften taken in combination with isoniazid.\nPyrazinamide (PZA) – 25 mg/kg per day\nTypically used for the first 2 months of treatment.\nEthambutol (EMB) – 15 mg/kg per day\nMay be added for the initial phase, particularly in cases with unknown resistance.\nPrescription Instructions:\n\nDosage:\nIsoniazid: 300 mg per day\nRifampin: 600 mg per day\nPyrazinamide: 25 mg/kg per day (approximately 700 mg for 28 kg)\nEthambutol: 15 mg/kg per day (approximately 420 mg for 28 kg)\nFrequency: Taken daily for 6 months, with supervision during the first 2 months.\nSide Effects:\nPotential side effects include nausea, vomiting, liver toxicity (especially with isoniazid and rifampin), and optic neuropathy (with ethambutol).\nAlcohol Consumption: Alcohol should be avoided during the treatment period to prevent liver damage.\nAdherence: It is crucial to complete the full course of treatment, even if symptoms improve, to prevent the development of drug-resistant TB.\nAlternative Treatment for Drug-Resistant TB:\nIf drug resistance is confirmed, a second-line treatment regimen involving more powerful drugs like fluoroquinolones and injectable agents may be required, extending treatment duration to over 12 months.\n\nMonitoring During Treatment\nRegular follow-up visits are crucial to monitor the effectiveness of the treatment and to check for any side effects. The following tests may be conducted during treatment:\n\nMonthly sputum smear and culture: To check for continued presence of Mycobacterium tuberculosis.\nLiver function tests: To monitor for hepatotoxicity, especially with drugs like isoniazid and rifampin.\nChest X-ray: To assess progress and resolution of pulmonary TB.\nVisual Acuity Test: To check for optic neuropathy due to ethambutol.\nAdditional Supportive Measures\nIn addition to medication, patients may benefit from supportive therapies, including:\n\nNutritional support: Encourage a high-protein, high-calorie diet to counteract weight loss associated with TB.\nCounseling: Psychological support to help Amina cope with the disease, treatment regimen, and isolation requirements.\nPrevention of Spread: Amina should wear a mask in public and avoid close contact with others, especially children and individuals with weakened immune systems.\nPrevention\nPreventive measures include:\n\nBCG Vaccine: Amina has already received the BCG vaccine during childhood.\nContact Screening: Family members and close contacts, including her husband, should be tested for TB.\nConclusion\nTuberculosis is a treatable condition, but it requires a comprehensive approach including accurate diagnosis, adherence to prescribed medication, and regular monitoring. Early detection, treatment initiation, and prevention strategies play a crucial role in controlling TB and minimizing the risk of transmission to others.\n\nAmina is expected to follow the prescribed treatment regimen closely and attend follow-up visits regularly to ensure the best outcomes.\n\nPrescribed Medication:\n\nIsoniazid (INH): 300 mg per day\nRifampin (RIF): 600 mg per day\nPyrazinamide (PZA): 700 mg per day\nEthambutol (EMB): 420 mg per day\nPatient's Next Appointment: 2025-02-22\n\nPrescribed By: Dr. Samira Nasri\n\n",
                "display_name": "Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Message to be passed as input.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "sender": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "Machine",
                  "User"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "sender",
                "value": "User",
                "display_name": "Sender Type",
                "advanced": true,
                "dynamic": false,
                "info": "Type of sender.",
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "sender_name": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "sender_name",
                "value": "User",
                "display_name": "Sender Name",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Name of the sender.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "session_id": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "session_id",
                "value": "",
                "display_name": "Session ID",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "should_store_message": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "should_store_message",
                "value": true,
                "display_name": "Store Messages",
                "advanced": true,
                "dynamic": false,
                "info": "Store the message in the history.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "text_color": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text_color",
                "value": "",
                "display_name": "Text Color",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The text color of the name",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Get chat inputs from the Playground.",
            "icon": "MessagesSquare",
            "base_classes": [
              "Message"
            ],
            "display_name": "Chat Input",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "message",
                "display_name": "Message",
                "method": "message_response",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "files",
              "background_color",
              "chat_icon",
              "text_color"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.1.1"
          },
          "type": "ChatInput",
          "id": "ChatInput-kaAiz"
        },
        "selected": false,
        "width": 300,
        "height": 218,
        "positionAbsolute": {
          "x": -444.15599999999984,
          "y": 44.35200000000003
        },
        "dragging": false
      },
      {
        "id": "Prompt-pZ53J",
        "type": "genericNode",
        "position": {
          "x": -90.54000000000002,
          "y": 37.224000000000046
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "tool_mode": false,
                "trace_as_input": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "okay so you are a summary ai that explain to the patients their documents and prescriptions and you are talking to non educated person , so i will give you {input} and you will explain it and summarize it . ",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput"
              },
              "input": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "input",
                "display_name": "input",
                "advanced": false,
                "input_types": [
                  "Message",
                  "Text"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt",
            "documentation": "",
            "custom_fields": {
              "template": [
                "input"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": null
              }
            ],
            "field_order": [
              "template"
            ],
            "beta": false,
            "legacy": false,
            "error": null,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.1.1"
          },
          "type": "Prompt",
          "id": "Prompt-pZ53J"
        },
        "selected": false,
        "width": 300,
        "height": 323,
        "positionAbsolute": {
          "x": -90.54000000000002,
          "y": 37.224000000000046
        },
        "dragging": false
      },
      {
        "id": "ChatOutput-HnynC",
        "type": "genericNode",
        "position": {
          "x": 875.2479999999998,
          "y": -137.4639999999999
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "background_color": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "background_color",
                "value": "",
                "display_name": "Background Color",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The background color of the icon.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "chat_icon": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "chat_icon",
                "value": "",
                "display_name": "Icon",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The icon of the message.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageInput, MessageTextInput, Output\nfrom langflow.schema.message import Message\nfrom langflow.schema.properties import Source\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, _id: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if _id:\n            source_dict[\"id\"] = _id\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            source_dict[\"source\"] = source\n        return Source(**source_dict)\n\n    def message_response(self) -> Message:\n        _source, _icon, _display_name, _source_id = self.get_properties_from_source_component()\n        _background_color = self.background_color\n        _text_color = self.text_color\n        if self.chat_icon:\n            _icon = self.chat_icon\n        message = self.input_value if isinstance(self.input_value, Message) else Message(text=self.input_value)\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        message.session_id = self.session_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(_source_id, _display_name, _source)\n        message.properties.icon = _icon\n        message.properties.background_color = _background_color\n        message.properties.text_color = _text_color\n        if self.session_id and isinstance(message, Message) and self.should_store_message:\n            stored_message = self.send_message(\n                message,\n            )\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "data_template": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "data_template",
                "value": "{text}",
                "display_name": "Data Template",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "input_value": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Message to be passed as output.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "sender": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "Machine",
                  "User"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "sender",
                "value": "Machine",
                "display_name": "Sender Type",
                "advanced": true,
                "dynamic": false,
                "info": "Type of sender.",
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "sender_name": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "sender_name",
                "value": "AI",
                "display_name": "Sender Name",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Name of the sender.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "session_id": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "session_id",
                "value": "",
                "display_name": "Session ID",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "should_store_message": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "should_store_message",
                "value": true,
                "display_name": "Store Messages",
                "advanced": true,
                "dynamic": false,
                "info": "Store the message in the history.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "text_color": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text_color",
                "value": "",
                "display_name": "Text Color",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The text color of the name",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Display a chat message in the Playground.",
            "icon": "MessagesSquare",
            "base_classes": [
              "Message"
            ],
            "display_name": "Chat Output",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "message",
                "display_name": "Message",
                "method": "message_response",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "data_template",
              "background_color",
              "chat_icon",
              "text_color"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.1.1"
          },
          "type": "ChatOutput",
          "id": "ChatOutput-HnynC"
        },
        "selected": false,
        "width": 300,
        "height": 218,
        "positionAbsolute": {
          "x": 875.2479999999998,
          "y": -137.4639999999999
        },
        "dragging": false
      },
      {
        "id": "OllamaModel-VmEUg",
        "type": "genericNode",
        "position": {
          "x": 303.28000000000014,
          "y": -173.7519999999999
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "output_parser": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "output_parser",
                "value": "",
                "display_name": "Output Parser",
                "advanced": true,
                "input_types": [
                  "OutputParser"
                ],
                "dynamic": false,
                "info": "The parser to use to parse the output of the model",
                "title_case": false,
                "type": "other",
                "_input_type": "HandleInput"
              },
              "base_url": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "base_url",
                "value": "http://localhost:11434",
                "display_name": "Base URL",
                "advanced": false,
                "dynamic": false,
                "info": "Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_ollama import ChatOllama\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs.inputs import HandleInput\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value, field_name)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value)\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/api/tags\")\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"name\"] for model in data.get(\"models\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure Ollama is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"llama3.1\",\n            info=\"Refer to https://ollama.com/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\", display_name=\"Format\", info=\"Specify the format of the output (e.g., json).\", advanced=True\n        ),\n        DictInput(name=\"metadata\", display_name=\"Metadata\", info=\"Metadata to add to the run trace.\", advanced=True),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(name=\"tfs_z\", display_name=\"TFS Z\", info=\"Tail free sampling value. (Default: 1)\", advanced=True),\n        IntInput(name=\"timeout\", display_name=\"Timeout\", info=\"Timeout for the request stream.\", advanced=True),\n        IntInput(\n            name=\"top_k\", display_name=\"Top K\", info=\"Limits token selection to top K. (Default: 40)\", advanced=True\n        ),\n        FloatInput(name=\"top_p\", display_name=\"Top P\", info=\"Works together with top-k. (Default: 0.9)\", advanced=True),\n        BoolInput(name=\"verbose\", display_name=\"Verbose\", info=\"Whether to print out response text.\", advanced=True),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(name=\"system\", display_name=\"System\", info=\"System to use for generating text.\", advanced=True),\n        StrInput(name=\"template\", display_name=\"Template\", info=\"Template to use for generating text.\", advanced=True),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n        *LCModelComponent._base_inputs,\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model_name,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)\n        except Exception as e:\n            msg = \"Could not initialize Ollama LLM.\"\n            raise ValueError(msg) from e\n\n        return output\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "format": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "format",
                "value": "",
                "display_name": "Format",
                "advanced": true,
                "dynamic": false,
                "info": "Specify the format of the output (e.g., json).",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "input_value": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "metadata": {
                "trace_as_input": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "metadata",
                "value": {},
                "display_name": "Metadata",
                "advanced": true,
                "dynamic": false,
                "info": "Metadata to add to the run trace.",
                "title_case": false,
                "type": "dict",
                "_input_type": "DictInput"
              },
              "mirostat": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "Disabled",
                  "Mirostat",
                  "Mirostat 2.0"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "mirostat",
                "value": "Disabled",
                "display_name": "Mirostat",
                "advanced": true,
                "dynamic": false,
                "info": "Enable/disable Mirostat sampling for controlling perplexity.",
                "real_time_refresh": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "mirostat_eta": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "mirostat_eta",
                "value": "",
                "display_name": "Mirostat Eta",
                "advanced": true,
                "dynamic": false,
                "info": "Learning rate for Mirostat algorithm. (Default: 0.1)",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              },
              "mirostat_tau": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "mirostat_tau",
                "value": "",
                "display_name": "Mirostat Tau",
                "advanced": true,
                "dynamic": false,
                "info": "Controls the balance between coherence and diversity of the output. (Default: 5.0)",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              },
              "model_name": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "deepseek-r1:1.5b",
                  "llava-llama3:latest"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "deepseek-r1:1.5b",
                "display_name": "Model Name",
                "advanced": false,
                "dynamic": false,
                "info": "Refer to https://ollama.com/library for more models.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "num_ctx": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "num_ctx",
                "value": "",
                "display_name": "Context Window Size",
                "advanced": true,
                "dynamic": false,
                "info": "Size of the context window for generating tokens. (Default: 2048)",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "num_gpu": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "num_gpu",
                "value": "",
                "display_name": "Number of GPUs",
                "advanced": true,
                "dynamic": false,
                "info": "Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "num_thread": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "num_thread",
                "value": "",
                "display_name": "Number of Threads",
                "advanced": true,
                "dynamic": false,
                "info": "Number of threads to use during computation. (Default: detected for optimal performance)",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "repeat_last_n": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "repeat_last_n",
                "value": "",
                "display_name": "Repeat Last N",
                "advanced": true,
                "dynamic": false,
                "info": "How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "repeat_penalty": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "repeat_penalty",
                "value": "",
                "display_name": "Repeat Penalty",
                "advanced": true,
                "dynamic": false,
                "info": "Penalty for repetitions in generated text. (Default: 1.1)",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              },
              "stop_tokens": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stop_tokens",
                "value": "",
                "display_name": "Stop Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "Comma-separated list of tokens to signal the model to stop generating text.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "stream": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": false,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system",
                "value": "",
                "display_name": "System",
                "advanced": true,
                "dynamic": false,
                "info": "System to use for generating text.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "system_message": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "tags": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tags",
                "value": "",
                "display_name": "Tags",
                "advanced": true,
                "dynamic": false,
                "info": "Comma-separated list of tags to add to the run trace.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "temperature": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.2,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls the creativity of model responses.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              },
              "template": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "",
                "display_name": "Template",
                "advanced": true,
                "dynamic": false,
                "info": "Template to use for generating text.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "tfs_z": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tfs_z",
                "value": "",
                "display_name": "TFS Z",
                "advanced": true,
                "dynamic": false,
                "info": "Tail free sampling value. (Default: 1)",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              },
              "timeout": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "timeout",
                "value": "",
                "display_name": "Timeout",
                "advanced": true,
                "dynamic": false,
                "info": "Timeout for the request stream.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_k": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Limits token selection to top K. (Default: 40)",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "Works together with top-k. (Default: 0.9)",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              },
              "verbose": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "verbose",
                "value": false,
                "display_name": "Verbose",
                "advanced": true,
                "dynamic": false,
                "info": "Whether to print out response text.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              }
            },
            "description": "Generate text using Ollama Local LLMs.",
            "icon": "Ollama",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "Ollama",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "hidden": null,
                "display_name": "Text",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": []
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "hidden": null,
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": []
              }
            ],
            "field_order": [
              "base_url",
              "model_name",
              "temperature",
              "format",
              "metadata",
              "mirostat",
              "mirostat_eta",
              "mirostat_tau",
              "num_ctx",
              "num_gpu",
              "num_thread",
              "repeat_last_n",
              "repeat_penalty",
              "tfs_z",
              "timeout",
              "top_k",
              "top_p",
              "verbose",
              "tags",
              "stop_tokens",
              "system",
              "template",
              "output_parser",
              "input_value",
              "system_message",
              "stream"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.1.1"
          },
          "type": "OllamaModel",
          "id": "OllamaModel-VmEUg"
        },
        "selected": false,
        "width": 300,
        "height": 626,
        "positionAbsolute": {
          "x": 303.28000000000014,
          "y": -173.7519999999999
        },
        "dragging": false
      },
      {
        "id": "GoogleSearchAPI-o9fPR",
        "type": "genericNode",
        "position": {
          "x": 50.99200000000015,
          "y": 503.62399999999997
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langchain_core.tools import Tool\n\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.inputs import IntInput, MultilineInput, SecretStrInput\nfrom langflow.schema import Data\n\n\nclass GoogleSearchAPIComponent(LCToolComponent):\n    display_name = \"Google Search API\"\n    description = \"Call Google Search API.\"\n    name = \"GoogleSearchAPI\"\n    icon = \"Google\"\n    inputs = [\n        SecretStrInput(name=\"google_api_key\", display_name=\"Google API Key\", required=True),\n        SecretStrInput(name=\"google_cse_id\", display_name=\"Google CSE ID\", required=True),\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Input\",\n        ),\n        IntInput(name=\"k\", display_name=\"Number of results\", value=4, required=True),\n    ]\n\n    def run_model(self) -> Data | list[Data]:\n        wrapper = self._build_wrapper()\n        results = wrapper.results(query=self.input_value, num_results=self.k)\n        data = [Data(data=result, text=result[\"snippet\"]) for result in results]\n        self.status = data\n        return data\n\n    def build_tool(self) -> Tool:\n        wrapper = self._build_wrapper()\n        return Tool(\n            name=\"google_search\",\n            description=\"Search Google for recent results.\",\n            func=wrapper.run,\n        )\n\n    def _build_wrapper(self):\n        try:\n            from langchain_google_community import GoogleSearchAPIWrapper\n        except ImportError as e:\n            msg = \"Please install langchain-google-community to use GoogleSearchAPIWrapper.\"\n            raise ImportError(msg) from e\n        return GoogleSearchAPIWrapper(google_api_key=self.google_api_key, google_cse_id=self.google_cse_id, k=self.k)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "google_api_key": {
                "load_from_db": false,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "google_api_key",
                "value": "",
                "display_name": "Google API Key",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "google_cse_id": {
                "load_from_db": false,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "google_cse_id",
                "value": "",
                "display_name": "Google CSE ID",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "input_value": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "k": {
                "trace_as_metadata": true,
                "list": false,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "k",
                "value": 4,
                "display_name": "Number of results",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              }
            },
            "description": "Call Google Search API.",
            "icon": "Google",
            "base_classes": [
              "Data",
              "Tool"
            ],
            "display_name": "Google Search API",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Data"
                ],
                "selected": "Data",
                "name": "api_run_model",
                "display_name": "Data",
                "method": "run_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "google_api_key",
                  "google_cse_id",
                  "k"
                ]
              },
              {
                "types": [
                  "Tool"
                ],
                "selected": "Tool",
                "name": "api_build_tool",
                "display_name": "Tool",
                "method": "build_tool",
                "value": "__UNDEFINED__",
                "cache": true,
                "required_inputs": [
                  "google_api_key",
                  "google_cse_id",
                  "k"
                ]
              }
            ],
            "field_order": [
              "google_api_key",
              "google_cse_id",
              "input_value",
              "k"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false
          },
          "type": "GoogleSearchAPI",
          "id": "GoogleSearchAPI-o9fPR"
        },
        "selected": false,
        "width": 300,
        "height": 504
      }
    ],
    "edges": [
      {
        "source": "OllamaModel-VmEUg",
        "sourceHandle": "{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-VmEUgœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ChatOutput-HnynC",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-HnynCœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-HnynC",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "OllamaModel",
            "id": "OllamaModel-VmEUg",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-OllamaModel-VmEUg{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-VmEUgœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-HnynC{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-HnynCœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "animated": false,
        "className": ""
      },
      {
        "source": "Prompt-pZ53J",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-pZ53Jœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "target": "OllamaModel-VmEUg",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œOllamaModel-VmEUgœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "OllamaModel-VmEUg",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-pZ53J",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-Prompt-pZ53J{œdataTypeœ:œPromptœ,œidœ:œPrompt-pZ53Jœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OllamaModel-VmEUg{œfieldNameœ:œinput_valueœ,œidœ:œOllamaModel-VmEUgœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "animated": false,
        "className": ""
      },
      {
        "source": "ChatInput-kaAiz",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-kaAizœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-pZ53J",
        "targetHandle": "{œfieldNameœ:œinputœ,œidœ:œPrompt-pZ53Jœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input",
            "id": "Prompt-pZ53J",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-kaAiz",
            "name": "message",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-ChatInput-kaAiz{œdataTypeœ:œChatInputœ,œidœ:œChatInput-kaAizœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-pZ53J{œfieldNameœ:œinputœ,œidœ:œPrompt-pZ53Jœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "animated": false,
        "className": ""
      }
    ],
    "viewport": {
      "x": 431.65930083378896,
      "y": 113.58741239799255,
      "zoom": 0.4878748479431073
    }
  },
  "gradient": null,
  "user_id": "d0b1a43a-f83d-48c6-be99-c31c2d7bb462",
  "is_component": false,
  "icon": null,
  "tags": null
}